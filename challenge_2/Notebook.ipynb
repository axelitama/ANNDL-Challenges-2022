{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks and Deep Learning 2022 - Homework 2\n",
    "### Politecnico di Milano\n",
    "Team name: bisogna_tenersi_idratati_nelle_ore_pi√π_calde\n",
    "\n",
    "Team members:\n",
    "- Alex Amati\n",
    "- Stefano Civelli\n",
    "- Luca Molteni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train 3 models and then we will perform voting between them at predict time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define all the parameters we need to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "seed = 0x911\n",
    "dir_models = 'models'\n",
    "\n",
    "params = {\n",
    "    'luca': {\n",
    "        'window' : 34,\n",
    "        'stride' : 1,\n",
    "        'val_stride': 3,\n",
    "        'features': ['std', 'norm', 'singlenorm', 'logstd'],\n",
    "        'epochs' : 200,\n",
    "        'ES_patience' : 50,\n",
    "        'LR_patience' : 15,\n",
    "        'LR_factor' : 0.5,\n",
    "        'LR_min' : 1e-10,\n",
    "        'LR' : 0.0001,\n",
    "        'batch_size' : 64,\n",
    "        'class_weight' : None\n",
    "    },\n",
    "    'ste': {\n",
    "        'window' : 36,\n",
    "        'stride' : 1,\n",
    "        'val_stride': 1,\n",
    "        'features' : ['std', 'norm', 'singlenorm', 'logstd'],\n",
    "        'epochs' : 200,\n",
    "        'ES_patience' : 50,\n",
    "        'LR_patience' : 15,\n",
    "        'LR_factor' : 0.5,\n",
    "        'LR_min' : 1e-10,\n",
    "        'LR' : 0.0001,\n",
    "        'batch_size' : 64,\n",
    "        'class_weight' : None\n",
    "    },\n",
    "    'alex': {\n",
    "        'window': 36,\n",
    "        'stride': 1,\n",
    "        'val_stride': 1,\n",
    "        'features': ['nothing'],\n",
    "        'epochs' : 200,\n",
    "        'ES_patience' : 50,\n",
    "        'LR_patience' : 15,\n",
    "        'LR_factor' : 0.5,\n",
    "        'LR_min' : 1e-10,\n",
    "        'LR' : 0.001,\n",
    "        'batch_size' : 64,\n",
    "        'class_weight' : None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "In this section we will load the dataset and we will perform all the neccessary trasformations to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('../x_train.npy')\n",
    "y = np.load('../y_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target in one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot = np.zeros((y.shape[0], y.max() + 1))\n",
    "oneHot[np.arange(y.shape[0]), y] = 1\n",
    "y = oneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the input in train set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_tmp, x_val_tmp, y_train_tmp, y_val_tmp = train_test_split(x, y, test_size=val_size, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use our simple library:\n",
    "- to perform and concatenate as features the transformations specified in params[model_name]['features']\n",
    "- to reduce the input window performing oversampling according to params[model_name]['window', 'stride', 'val_stride']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libreriabella import preprocess_data\n",
    "x_train = {}; x_val = {}; y_train = {}; y_val = {}\n",
    "\n",
    "preproc = preprocess_data()\n",
    "\n",
    "x_train['luca'], x_val['luca'], y_train['luca'], y_val['luca'] = preproc.preprocess_train(x_train_tmp, x_val_tmp, y_train_tmp, y_val_tmp, params['luca']['window'], params['luca']['stride'], params['luca']['val_stride'], params['luca']['features'])\n",
    "x_train['ste'], x_val['ste'], y_train['ste'], y_val['ste'] = preproc.preprocess_train(x_train_tmp, x_val_tmp, y_train_tmp, y_val_tmp, params['ste']['window'], params['ste']['stride'], params['ste']['val_stride'], params['ste']['features'])\n",
    "x_train['alex'], x_val['alex'], y_train['alex'], y_val['alex'] = preproc.preprocess_train(x_train_tmp, x_val_tmp, y_train_tmp, y_val_tmp, params['alex']['window'], params['alex']['stride'], params['alex']['val_stride'], params['alex']['features'])\n",
    "\n",
    "predictParams = preproc.getPredictParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = {}\n",
    "\n",
    "input_shape['luca'] = x_train['luca'].shape[1:]\n",
    "input_shape['ste'] = x_train['ste'].shape[1:]\n",
    "input_shape['alex'] = x_train['alex'].shape[1:]\n",
    "\n",
    "classes = y_train['luca'].shape[-1] # number of classes is always the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section we will provide the code to build and train our three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_luca(input_shape, classes, seed, lr=0.001):\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='luca_input')\n",
    "    layer = tfkl.GaussianDropout(0.4, seed=seed, name='luca_gdropout')(input_layer)\n",
    "    layer = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='luca_conv0', kernel_initializer=tfk.initializers.GlorotUniform(seed))(layer)\n",
    "    layer = tfkl.MaxPooling1D(2, name='luca_pool0')(layer)\n",
    "    layer = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='luca_conv1', kernel_initializer=tfk.initializers.GlorotUniform(seed))(layer)\n",
    "    gap1 = tfkl.GlobalAveragePooling1D(name='luca_gap1')(layer)\n",
    "    layer = tfkl.MaxPooling1D(2, name='luca_pool1')(layer)\n",
    "    layer = tfkl.Conv1D(16, 3, padding='same', activation='relu', name='luca_conv2', kernel_initializer=tfk.initializers.GlorotUniform(seed))(layer)\n",
    "    gap0 = tfkl.GlobalAveragePooling1D(name='luca_gap0')(layer)\n",
    "\n",
    "    concat = tfkl.Concatenate(name='luca_concat')([gap0, gap1])\n",
    "    dropout =tfkl.Dropout(0.3, seed=seed, name='luca_dropout')(concat)\n",
    "    classifier = tfkl.Dense(128, activation='relu', name='luca_dense', kernel_initializer=tfk.initializers.GlorotUniform(seed))(dropout)\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax', name='luca_output', kernel_initializer=tfk.initializers.GlorotUniform(seed))(classifier)\n",
    "\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='luca_model')\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(lr), metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ste(input_shape, classes, seed, lr=0.001):\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='luca_input')\n",
    "    gdropout = tfkl.GaussianDropout(0.4, seed=seed, name='luca_gdropout')(input_layer)\n",
    "\n",
    "    layer0 = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='ste_conv0-0', kernel_initializer=tfk.initializers.GlorotUniform(seed))(gdropout)\n",
    "    layer0 = tfkl.MaxPooling1D(2, name='ste_pool0')(layer0)\n",
    "    layer0 = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='ste_conv0-1', kernel_initializer=tfk.initializers.GlorotUniform(seed))(layer0)\n",
    "    layer0 = tfkl.GlobalAveragePooling1D(name='ste_gap0')(layer0)\n",
    "    layer0 = tfkl.Dropout(0.5, seed=seed, name='ste_dropout0')(layer0)\n",
    "\n",
    "    layer1 = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='ste_conv1-0', kernel_initializer=tfk.initializers.GlorotUniform(seed))(gdropout)\n",
    "    layer1 = tfkl.MaxPooling1D(2, name='ste_pool1')(layer1)\n",
    "    layer1 = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='ste_conv1-1', kernel_initializer=tfk.initializers.GlorotUniform(seed))(layer1)\n",
    "    layer1 = tfkl.GlobalAveragePooling1D(name='ste_gap1')(layer1)\n",
    "    layer1 = tfkl.Dropout(0.5, seed=seed, name='ste_dropout1')(layer1)\n",
    "\n",
    "    layer2 = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='ste_conv2-0', kernel_initializer=tfk.initializers.GlorotUniform(seed))(gdropout)\n",
    "    layer2 = tfkl.MaxPooling1D(2, name='ste_pool2')(layer2)\n",
    "    layer2 = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='ste_conv2-1', kernel_initializer=tfk.initializers.GlorotUniform(seed))(layer2)\n",
    "    layer2 = tfkl.GlobalAveragePooling1D(name='ste_gap2')(layer2)\n",
    "    layer2 = tfkl.Dropout(0.5, seed=seed, name='ste_dropout2')(layer2)\n",
    "\n",
    "    concat = tfkl.Concatenate(name='ste_concat')([layer0, layer1, layer2])\n",
    "    classifier = tfkl.Dense(128, activation='relu', name='ste_dense', kernel_initializer=tfk.initializers.GlorotUniform(seed))(concat)\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax', name='ste_output', kernel_initializer=tfk.initializers.GlorotUniform(seed))(classifier)\n",
    "\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='ste_model')\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(lr), metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alex(input_shape, classes, seed, lr=0.001):\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='alex_input')\n",
    "\n",
    "    cnn = tfkl.Conv1D(512, 3, padding='same', activation='relu', name='alex_b0conv0', kernel_initializer=tfk.initializers.GlorotUniform(seed))(input_layer)\n",
    "    cnn = tfkl.MaxPooling1D(2, name='alex_pool0')(cnn)\n",
    "    cnn = tfkl.Conv1D(256, 3, padding='same', activation='relu', name='alex_b1conv0', kernel_initializer=tfk.initializers.GlorotUniform(seed))(cnn)\n",
    "    cnn = tfkl.Conv1D(256, 3, padding='same', activation='relu', name='alex_b1conv1', kernel_initializer=tfk.initializers.GlorotUniform(seed))(cnn)\n",
    "    cnn = tfkl.Conv1D(256, 3, padding='same', activation='relu', name='alex_b1conv2', kernel_initializer=tfk.initializers.GlorotUniform(seed))(cnn)\n",
    "    cnn = tfkl.MaxPooling1D(2, name='alex_pool1')(cnn)\n",
    "    cnn = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='alex_b2conv0', kernel_initializer=tfk.initializers.GlorotUniform(seed))(cnn)\n",
    "    cnn = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='alex_b2conv1', kernel_initializer=tfk.initializers.GlorotUniform(seed))(cnn)\n",
    "    cnn = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='alex_b2conv2', kernel_initializer=tfk.initializers.GlorotUniform(seed))(cnn)\n",
    "    gap = tfkl.GlobalAveragePooling1D(name='alex_gap')(cnn)\n",
    "\n",
    "    dropout =tfkl.Dropout(0.3, seed=seed, name='alex_dropout')(gap)\n",
    "    classifier = tfkl.Dense(128, activation='relu', name='alex_dense', kernel_initializer=tfk.initializers.GlorotUniform(seed))(dropout)\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax', name='alex_output', kernel_initializer=tfk.initializers.GlorotUniform(seed))(classifier)\n",
    "\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='alex_model')\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(lr), metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "models['luca'] = build_luca(input_shape['luca'], classes, seed, params['luca']['LR'])\n",
    "models['ste'] = build_ste(input_shape['ste'], classes, seed, params['ste']['LR'])\n",
    "models['alex'] = build_alex(input_shape['alex'], classes, seed, params['alex']['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['luca'].summary()\n",
    "# tfk.utils.plot_model(models['luca'], expand_nested=True,  show_shapes=True, show_dtype=False, show_layer_names=True, show_layer_activations=True, to_file=models['luca'].name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['ste'].summary()\n",
    "# tfk.utils.plot_model(models['ste'], expand_nested=True,  show_shapes=True, show_dtype=False, show_layer_names=True, show_layer_activations=True, to_file=models['ste'].name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['alex'].summary()\n",
    "# tfk.utils.plot_model(models['alex'], expand_nested=True,  show_shapes=True, show_dtype=False, show_layer_names=True, show_layer_activations=True, to_file=models['alex'].name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "for key, model in models.items():\n",
    "    history[key] = model.fit(\n",
    "        x = x_train[key],\n",
    "        y = y_train[key],\n",
    "        validation_data = (x_val[key], y_val[key]),\n",
    "        shuffle = True,\n",
    "        epochs = params[key]['epochs'],\n",
    "        batch_size = params[key]['batch_size'],\n",
    "        class_weight = params[key]['class_weight'],\n",
    "        callbacks = [\n",
    "            tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=params[key]['ES_patience'], restore_best_weights=True),\n",
    "            tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=params[key]['LR_patience'], factor=params[key][\"LR_factor\"], min_lr=params[key]['LR_min'])\n",
    "        ]\n",
    "    ).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, model in models.items():\n",
    "#     model.save(os.path.join(dir_models, key))\n",
    "\n",
    "#     hist_df = pd.DataFrame(history[key])\n",
    "#     hist_csv_file = os.path.join(dir_models, key, 'history.csv')\n",
    "#     with open(hist_csv_file, mode='w') as f:\n",
    "#         hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sulauzytas - majority voting\n",
    "Here we define our class to perform the majority voting between the three previous models.\n",
    "Note that the model.py we uploaded on CodaLab was different because for time purpose we used a less general approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libreriabella import preprocess_data\n",
    "\n",
    "class Sulauzytas:\n",
    "    def __init__(self, model1, features1, model2, features2, model3, features3, params):\n",
    "        self._preproc = preprocess_data()\n",
    "        self._preproc.setPredictParams(*params)\n",
    "\n",
    "        self._models = [model1, model2,model3]\n",
    "        self._features = [features1, features2, features3]\n",
    "\n",
    "    def _voteBetween3WindowsSamples(self, _out):\n",
    "        out = []\n",
    "        for a, b, c in zip(_out[0::3], _out[1::3], _out[2::3]):\n",
    "            if a == b and b == c:\n",
    "                out.append(a)\n",
    "            elif a == b:\n",
    "                out.append(a)\n",
    "            elif b == c:\n",
    "                out.append(b)\n",
    "            elif a == c:\n",
    "                out.append(a)\n",
    "            else:\n",
    "                out.append(a)\n",
    "        return np.array(out)\n",
    "\n",
    "    def predict(self, x):\n",
    "        for i, model in enumerate(self._models):\n",
    "            assert (x.shape[1] == model.input_shape[1]) or (x.shape[1] - model.input_shape[1] == 2), \"model\"+str(i+1)+\": (input_window - model_window) not in [0, 2] --- NOT IMPLEMENTED\"\n",
    "        \n",
    "        inputs = []\n",
    "        for i, model in enumerate(self._models):\n",
    "            inputs.append(self._preproc.preprocess_predict(x, model.input_shape[1], 1, self._features[i]))\n",
    "\n",
    "        outputs = []\n",
    "        for i, (model, input) in enumerate(zip(self._models, inputs)):\n",
    "            _out = model.predict(input)\n",
    "            _out = tf.argmax(_out, axis=-1)\n",
    "            if (model.input_shape[1] == x.shape[1] - 2):\n",
    "                _out = self._voteBetween3WindowsSamples(_out)\n",
    "            outputs.append(_out)\n",
    "        \n",
    "        a = np.array(outputs[0])\n",
    "        b = np.array(outputs[1])\n",
    "        c = np.array(outputs[2])\n",
    "\n",
    "        prediction = []\n",
    "        # We found using the confsion matrix that class 7 was almost always correctly predicted by the second model (b)\n",
    "        for i in range(len(a)):\n",
    "            if(b[i] == 7):\n",
    "                prediction.append(b[i])\n",
    "            elif (a[i] == b[i] or a[i] == c[i]):\n",
    "                prediction.append(a[i])\n",
    "            elif (b[i] == c[i]):\n",
    "                prediction.append(c[i])\n",
    "            elif (a[i] != b[i] != c[i]):\n",
    "                prediction.append(a[i])\n",
    "\n",
    "        numpy_pred = np.array(prediction)\n",
    "        tensor_pred = tf.convert_to_tensor(numpy_pred)\n",
    "\n",
    "        return tensor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sulauzytas = Sulauzytas(models['ste'], params['ste']['features'], models['luca'], params['luca']['features'], models['alex'], params['alex']['features'], predictParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did't create a test set so we will try the predict function on our validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sulauzytas.predict(x_val_tmp)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
